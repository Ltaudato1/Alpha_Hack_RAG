# import csv
# import sys
# import os
# import pandas as pd
#
# sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
# sys.path.insert(0, '/app/src')
#
# from config.database import db
# from services.model_loader import load_embedding_model
# from services.embeder import Embeder
# from services.vector_store import VectorStore
# from services.retriever import Retriever
#
#
# def load_csv_texts(csv_path, num_texts=15):
#     """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ CSV —Ñ–∞–π–ª–∞"""
#     texts = []
#     try:
#         with open(csv_path, 'r', encoding='utf-8') as file:
#             reader = csv.DictReader(file)
#             for i, row in enumerate(reader):
#                 if i >= num_texts:
#                     break
#                 # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –≤ –∫–æ–ª–æ–Ω–∫–µ 'text' –∏–ª–∏ –ø–µ—Ä–≤–æ–π –∫–æ–ª–æ–Ω–∫–µ
#                 text = row.get('text', list(row.values())[0] if row else '')
#                 if text.strip():
#                     texts.append(text.strip())
#         print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ CSV")
#         return texts
#     except Exception as e:
#         print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}")
#         return []
#
#
# def main():
#     # 1. –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
#     db.connect()
#     db.init_tables()
#
#     # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
#     model = load_embedding_model()
#
#     # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å—ã
#     store = VectorStore()
#     embeder = Embeder(model, store)
#     retriever = Retriever()
#
#     # 4. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏–∑ CSV (–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —Ñ–∞–π–ª –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è texts.csv)
#     csv_file = data_w = pd.read_csv('websites_updated.csv')['text']  # –£–∫–∞–∂–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É CSV
#     texts = load_csv_texts(csv_file, 15)
#
#     if not texts:
#         print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç—ã –∏–∑ CSV")
#         return
#
#     # 5. –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
#     print("üîÑ –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î...")
#     document_ids = embeder.batch_embed_and_store(texts)
#     print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(document_ids)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ë–î")
#
#     # 6. –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
#     print("\nüîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫...")
#     test_queries = list(pd.read_csv('questions_clean.csv')['query'])
#     # test_queries = [
#     #     "–æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏",  # –ø—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞
#     #     "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
#     #     "–º–µ—Ç–æ–¥—ã –∞–Ω–∞–ª–∏–∑–∞"
#     # ]
#
#     for query in test_queries:
#         print(f"\n–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
#         try:
#             # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
#             query_embedding = embeder.generate_embedding(query)
#
#             # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
#             results = retriever.retrieve(query_embedding, limit=5)
#
#             print(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
#             for i, result in enumerate(results):
#                 print(f"  {i + 1}. –°—Ö–æ–¥—Å—Ç–≤–æ: {result['similarity']:.4f}")
#                 print(f"     –¢–µ–∫—Å—Ç: {result['content'][:100]}...")
#
#         except Exception as e:
#             print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
#
#     # 7. –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
#     db.disconnect()
#     print("\n‚úÖ –ó–∞–¥–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ!")
#
#
# if __name__ == "__main__":
#     main()

import csv
import sys
import os
import pandas as pd

sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
sys.path.insert(0, '/app/src')

from config.database import db
from services.model_loader import load_embedding_model
from services.embeder import Embeder
from services.vector_store import VectorStore
from services.retriever import Retriever


def load_csv_texts(csv_path, num_texts=15):
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ CSV —Ñ–∞–π–ª–∞"""
    texts = []
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º pandas –¥–ª—è —á—Ç–µ–Ω–∏—è CSV
        df = pd.read_csv(csv_path)

        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –≤ –∫–æ–ª–æ–Ω–∫–µ 'text'
        # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ 'text' –Ω–µ—Ç, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É
        if 'text' in df.columns:
            text_column = 'text'
        else:
            text_column = df.columns[0]

        # –ë–µ—Ä–µ–º —É–∫–∞–∑–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤
        texts = df[text_column].head(num_texts).dropna().tolist()
        texts = [str(text).strip() for text in texts if str(text).strip()]

        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ CSV")
        return texts
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}")
        return []


def main():
    # 1. –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
    db.connect()
    # –û–ß–ò–°–¢–ö–ê –ë–î –û–¢ –°–¢–ê–†–´–• –î–ê–ù–ù–´–•
    with db.connection.cursor() as cursor:
        cursor.execute("TRUNCATE TABLE text_embeddings RESTART IDENTITY;")
        db.connection.commit()
    print("üóëÔ∏è  –°—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —É–¥–∞–ª–µ–Ω—ã –∏–∑ –ë–î")
    db.init_tables()

    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model = load_embedding_model()

    # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å—ã
    store = VectorStore()
    embeder = Embeder(model, store)
    retriever = Retriever()

    # 4. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏–∑ CSV
    csv_file = 'websites_updated.csv'  # –ü—Ä–æ—Å—Ç–æ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
    texts = load_csv_texts(csv_file, 15)

    if not texts:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç—ã –∏–∑ CSV")
        return

    # 5. –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
    print("üîÑ –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î...")
    document_ids = []
    for i, text in enumerate(texts, 1):
        try:
            doc_id = embeder.embed_and_store(text, {"source": "websites", "index": i})
            if doc_id:
                document_ids.append(doc_id)
                print(f"‚úÖ –¢–µ–∫—Å—Ç {i} —Å–æ—Ö—Ä–∞–Ω–µ–Ω —Å ID: {doc_id}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ {i}: {e}")

    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(document_ids)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ë–î")

    # 6. –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
    print("\nüîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫...")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã –∏–∑ –¥—Ä—É–≥–æ–≥–æ CSV
    try:
        questions_df = pd.read_csv('questions_clean.csv')
        test_queries = questions_df['query'].head(3).tolist()  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 –≤–æ–ø—Ä–æ—Å–∞
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ questions_clean.csv: {e}")
        # Fallback - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        test_queries = [
            "–æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏",
            "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
            "–º–µ—Ç–æ–¥—ã –∞–Ω–∞–ª–∏–∑–∞"
        ]

    for query in test_queries:
        print(f"\n–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = embeder.generate_embedding(query)

            # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            results = retriever.retrieve(query_embedding, limit=5)

            print(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
            for i, result in enumerate(results):
                print(f"  {i + 1}. –°—Ö–æ–¥—Å—Ç–≤–æ: {result['similarity']:.4f}")
                print(f"     –¢–µ–∫—Å—Ç: {result['content'][:100]}...")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")

    # 7. –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
    db.disconnect()
    print("\n‚úÖ –ó–∞–¥–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ!")


if __name__ == "__main__":
    main()
