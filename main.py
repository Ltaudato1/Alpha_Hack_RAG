from src.config.database import db
import pandas as pd
import re
from psycopg2.extras import Json
from src.services.vector_store import VectorStore
from llama_cpp import Llama

from src.services.vector_store import VectorStore
from src.services.embeder import Embeder
from src.services.retriever import Retriever


import os


def load_embedding_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ GGUF –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    model_path = "models/jina-embeddings-v4-text-retrieval-IQ1_S.gguf"
    #model_path = "models/qodo-embed-1-1.5b-q4_k_m.gguf"

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {model_path}")
    
    model = Llama(
            model_path=model_path,
            embedding=True,
            n_threads=6,
            n_threads_batch=6,
            verbose=False
        )

    print("‚úÖ –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    return model

def chunk_text(text, chunk_size=250, overlap=50):
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
    
    Args:
        text (str): –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        chunk_size (int): —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–ª–æ–≤–∞—Ö
        overlap (int): –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –≤ —Å–ª–æ–≤–∞—Ö
    
    Returns:
        list: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤
    """
    if not text or not isinstance(text, str):
        return []
    
    # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    text = re.sub(r'\s+', ' ', text.strip())
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
    words = text.split()
    
    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—á–µ chunk_size, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
    if len(words) <= chunk_size:
        return [' '.join(words)] if words else []
    
    chunks = []
    start = 0
    
    while start < len(words):
        end = start + chunk_size
        chunk = ' '.join(words[start:end])
        chunks.append(chunk)
        
        start += chunk_size - overlap
        
        if start >= len(words):
            break
            
        remaining = len(words) - start
        if remaining < chunk_size and remaining > overlap:
            last_chunk = ' '.join(words[start:])
            chunks.append(last_chunk)
            break
    
    return chunks

def load_csv_texts(csv_path, num_texts=None, chunk_size=250, overlap=50, min_chunk_length=50):
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ CSV —Ñ–∞–π–ª–∞ —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ —á–∞–Ω–∫–∏"""
    
    texts = []
    idxs = []
    try:
        df = pd.read_csv(csv_path).dropna()
        text_column = 'text'
        id_column = 'web_id'

        if num_texts is None:
            num_texts = len(df)
        else:
            num_texts = min(num_texts, len(df))
        
        raw_texts = df[text_column].head(num_texts).tolist()
        raw_texts = [str(text).strip() for text in raw_texts if str(text).strip()]
        ids = df[id_column].head(num_texts).tolist()
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(raw_texts)} –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ CSV")
        
        total_chunks = 0
        verbose = 100
        for i, text in enumerate(raw_texts):
            chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)
            
            filtered_chunks = [
                chunk for chunk in chunks 
                if len(chunk.split()) >= min_chunk_length
            ]
            
            texts.extend(filtered_chunks)
            total_chunks += len(filtered_chunks)
            idxs.extend([{'id': ids[i]}] * len(filtered_chunks))
            
            if (i + 1) % verbose == 0:
                print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(raw_texts)} —Ç–µ–∫—Å—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–æ {total_chunks} —á–∞–Ω–∫–æ–≤")
        
        print(f"üéØ –ò—Ç–æ–≥: {len(raw_texts)} —Ç–µ–∫—Å—Ç–æ–≤ ‚Üí {len(texts)} —á–∞–Ω–∫–æ–≤")

        return texts, idxs

        
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}")
        return [], []

def generate_proper_submission():
    try:
        print("üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è submission.csv —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏...")

        model = load_embedding_model()
        embeder = Embeder(model, store=None)
        retriever = Retriever()

        questions_df = pd.read_csv('questions_clean.csv')
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(questions_df)} –≤–æ–ø—Ä–æ—Å–æ–≤")

        results = []

        for index, row in questions_df.iterrows():
            q_id = index + 1
            query = row['query']

            print(f"\nüîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å {q_id}: '{query}'")

            try:
                # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
                query_embedding = embeder.generate_embedding(query)

                # –ò—â–µ–º 5 —Å–∞–º—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                search_results = retriever.retrieve(query_embedding, limit=5)

                if search_results:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                    doc_ids = [result['metadata']['id'] for result in search_results]

                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                    web_list_str = "[" + ", ".join(map(str, doc_ids)) + "]"

                    results.append({
                        'q_id': q_id,
                        'web_list': web_list_str
                    })

                    print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(doc_ids)}")
                    print(f"   üìã ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {web_list_str}")

                    for i, result in enumerate(search_results):
                        print(f"      {i + 1}. ID {result['metadata']['id']}, —Å—Ö–æ–¥—Å—Ç–≤–æ: {result['similarity']:.4f}")
                else:
                    print(f"   ‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ {q_id}")
                    # Fallback - –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
                    results.append({
                        'q_id': q_id,
                        'web_list': "[]"
                    })

            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞ {q_id}: {e}")
                # Fallback - –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                results.append({
                    'q_id': q_id,
                    'web_list': "[]"
                })

        # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
        submission_df = pd.DataFrame(results)
        submission_df.to_csv('submission.csv', index=False)

        print(f"\nüéâ –§–∞–π–ª submission.csv —Å–æ–∑–¥–∞–Ω!")
        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(results)}")
        print(f"üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: /app/submission.csv")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        successful_queries = sum(1 for r in results if r['web_list'] != "[]")
        print(f"üìà –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {successful_queries}/{len(results)}")

        # –ü–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫
        print("\nüìÑ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ submission.csv:")
        print(submission_df.head(10))

    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if db.connection:
            db.disconnect()

def clean_db():
    with db.connection.cursor() as cursor:
        cursor.execute("TRUNCATE TABLE text_embeddings RESTART IDENTITY;")
        db.connection.commit()

def create_embeddings():
    clean_db()
    data, metadata = load_csv_texts(csv_path='websites.csv')
    
    store = VectorStore()
    emb_model = load_embedding_model()
    emb = Embeder(emb_model, store)
    emb.batch_embed_and_store(documents=data, metadata_list=metadata)



def main():
    db.connect()
    create_embeddings()
    generate_proper_submission()
    db.disconnect()
    
    

if __name__ == "__main__":
    main()